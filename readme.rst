=====================
同步,异步文件读取测试
=====================

文件读取测试模型分为同步，异步全量读取，分片读取；
同步读取使用linux系统调用open，read，wirte；
异步读取使用linux系统调用aio，其是linux提出的一套可支持异步读取的系统调用，
目前只支持direct_io方式启动，如果设置o_direct，其表现形式与同步相同；

工具使用方式
====================

数据准备阶段，如果没有测试文件，可以使用脚本生成::
    
    创建data目录，将create_copy移入data中，修改脚本中的文件大小及生成数量即可；

编译生成可执行文件，在根目录在make即可，生成async，sync可执行文件，参数说明及使用方式::
    
    sync_io_read: 支持同步全量读取，分片读取文件；

    async_io_read: 支持异步全量读取，分片读取文件；在此可执行文件中，不设定o_direct，
                可正常获取数据，相当于同步读取，但代码结构以及使用方式不同，详见代码；
                设置o_direct后，是异步读取模型，文件读取未阻塞，但由于反应时间过快，导
                致获取不到数据，为解决此问题，采用以下两种方式，thread及epoll监控方式；

    async_thread_read: 单独创建一个线程采用轮询的方式获取io_getevents，此时可解决数据
                获取不到的问题，但thread一直轮询，占用cpu；

    async_eventfd_read: aio提供eventfd事件通知，可以在使用时设置eventfd用以响应，这样就
                可以利用epoll监控eventfd来获取已完成的读取事件，以此来获取数据，目前，
                代码中是提交io_submit后，利用epoll完成将数据收取上来后，在进行下一次调用，
                因此cpu占用很低；

测试目的
====================

作为一个实时系统，要保证每20ms必须将数据发送出去，因此我们不能容忍进程被任何系统调用阻塞20ms
以上，为此，构建测试模型来测试文件读取的性能，测出linux磁盘读取性能，主要是为了测试linux提供
的异步aio是否能够满足我们的需求。

测试模型
====================

模型分为同步，异步，全量读取，分片读取；
全量读取为连续读取，读完后读下一个，直到读完，测出使用的时间；
分片读取为随机读取，按片读每个文件，直到读完全部文件，测出使用的时间；

测试结果
====================

同步读取会随机阻塞，由于每次read后，操作系统会进行预读数据到缓存中，导致调用时间不同；

异步读取发现io_submit系统调用其实会阻塞进程，是在进程内部同步运行的，导致我们不能使用
异步读取来设计；

注意事项
====================

#. aio目前只支持direct_io，如果没有设置O_DIRECT，表现形式与同步相同；

#. aio定义的结构iocb中的data，可以用于数据传送，其会io_getevents的时候复制到io_event
   的结构中，libaio封装的callback回调其实就是利用这一特性；

#. io_submit这个系统调用就算每次只提交一个读取请求，会偶尔阻塞20ms左右，这一点就很坑了
   要不然我们就可以使用这个方式去开发了；

#. eventfd epoll监控完成事件的时候，io_getevents内的参数最好设置每次只获取一个时间，如果
   同时获取多个，偶尔会出现数据收不上来，退出循环的时候导致不能退出，不排除是我程序有问题
